from crewai.tools import BaseTool
from pydantic import BaseModel, Field
from typing import Type
import json
from pathlib import Path
from .scanner import AgentSystemScanner
from .log_analyzer import ExecutionLogAnalyzer


class DirectoryScanInput(BaseModel):
    directory_path: str = Field(..., description="Directory path to scan")
    output_file: str = Field(default="scan_result.json", description="Output file name")


class FileScanInput(BaseModel):
    file_path: str = Field(..., description="File path to scan")
    output_file: str = Field(default="scan_result.json", description="Output file name")


class DirectoryScanTool(BaseTool):
    """Directory scan tool"""
    name: str = "directory_scanner"
    description: str = "Scan agent system components in directory"
    args_schema: Type[BaseModel] = DirectoryScanInput

    def _run(self, directory_path: str, output_file: str = "scan_result.json") -> str:
        try:
            scanner = AgentSystemScanner()
            result = scanner.scan_directory(directory_path)
            
            # Save result
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            summary = result['scan_summary']
            return f"Scan completed: {summary['total_agents']} agents, {summary['total_tools']} tools"
        
        except Exception as e:
            return f"Scan failed: {e}"


class FileScanTool(BaseTool):
    """File scan tool"""
    name: str = "file_scanner"
    description: str = "Scan agent components in a single file"
    args_schema: Type[BaseModel] = FileScanInput

    def _run(self, file_path: str, output_file: str = "scan_result.json") -> str:
        try:
            scanner = AgentSystemScanner()
            result = scanner.scan_file(file_path)
            
            # saveresult
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            summary = result['scan_summary']
            return f"æ–‡ä»¶æ‰«æå®Œæˆ: {summary['total_agents']} agents, {summary['total_tools']} tools"
        
        except Exception as e:
            return f"æ‰«æå¤±è´¥: {e}"
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            summary = result['scan_summary']
            return f"""
æ‰«æå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ° {output_file}

æ‰«ææ‘˜è¦:
- å‘ç° {summary['total_agents']} ä¸ª agents
- å‘ç° {summary['total_tools']} ä¸ª tools  
- å‘ç° {summary['total_crews']} ä¸ª crews
- å‘ç° {summary['total_tasks']} ä¸ª tasks
- æ€»å…±æ‰«æ {summary['total_files']} ä¸ªæ–‡ä»¶

è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹ {output_file} æ–‡ä»¶ã€‚
            """
        except Exception as e:
            return f"æ‰«æå¤±è´¥: {str(e)}"


class FileScanTool(BaseTool):
    """scanå•ä¸ªfileä¸­agentsystemçš„tool"""
    name: str = "file_scanner"
    description: str = "æ‰«ææŒ‡å®šæ–‡ä»¶ï¼Œåˆ†æå…¶ä¸­çš„agentç³»ç»Ÿç»“æ„ï¼Œè¯†åˆ«agentsã€toolsç­‰ç»„ä»¶"
    args_schema: Type[BaseModel] = FileScanInput

    def _run(self, file_path: str, output_file: str = "scan_result.json") -> str:
        try:
            scanner = AgentSystemScanner()
            result = scanner.scan_file(file_path)
            
            # saveresultåˆ°JSONfile
            output_path = Path(output_file)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            
            summary = result['scan_summary']
            return f"""
æ‰«æå®Œæˆ! ç»“æœå·²ä¿å­˜åˆ° {output_file}

æ‰«ææ‘˜è¦:
- å‘ç° {summary['total_agents']} ä¸ª agents
- å‘ç° {summary['total_tools']} ä¸ª tools  
- å‘ç° {summary['total_crews']} ä¸ª crews
- å‘ç° {summary['total_tasks']} ä¸ª tasks

è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹ {output_file} æ–‡ä»¶ã€‚
            """
        except Exception as e:
            return f"æ‰«æå¤±è´¥: {str(e)}"


class ReportAnalysisTool(BaseTool):
    """analyzescanreportçš„tool"""
    name: str = "report_analyzer"
    description: str = "åˆ†ææ‰«æç»“æœJSONæ–‡ä»¶ï¼Œæä¾›è¯¦ç»†çš„agentç³»ç»Ÿæ¶æ„åˆ†æ"
    
    class ReportAnalysisInput(BaseModel):
        report_file: str = Field(..., description="æ‰«æç»“æœJSONæ–‡ä»¶è·¯å¾„")
    
    args_schema: Type[BaseModel] = ReportAnalysisInput

    def _run(self, report_file: str) -> str:
        try:
            with open(report_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            analysis = []
            analysis.append("=== Agentç³»ç»Ÿæ¶æ„åˆ†æ ===\n")
            
            # analyzeagents
            if data['agents']:
                analysis.append("ğŸ¤– å‘ç°çš„Agents:")
                for agent in data['agents']:
                    analysis.append(f"  - {agent['name']} ({agent['type']}) - {agent['file']}")
                    if 'arguments' in agent and agent['arguments']:
                        for key, value in agent['arguments'].items():
                            analysis.append(f"    {key}: {value}")
                analysis.append("")
            
            # analyzetools
            if data['tools']:
                analysis.append("ğŸ”§ å‘ç°çš„Tools:")
                for tool in data['tools']:
                    analysis.append(f"  - {tool['name']} ({tool['type']}) - {tool['file']}")
                    if 'arguments' in tool and tool['arguments']:
                        for key, value in tool['arguments'].items():
                            analysis.append(f"    {key}: {value}")
                analysis.append("")
            
            # analyzecrews
            if data['crews']:
                analysis.append("ğŸ‘¥ å‘ç°çš„Crews:")
                for crew in data['crews']:
                    analysis.append(f"  - {crew['name']} ({crew['type']}) - {crew['file']}")
                analysis.append("")
            
            # analyzetasks
            if data['tasks']:
                analysis.append("ğŸ“‹ å‘ç°çš„Tasks:")
                for task in data['tasks']:
                    analysis.append(f"  - {task['name']} ({task['type']}) - {task['file']}")
                analysis.append("")
            
            # fileç»“æ„analyze
            analysis.append("ğŸ“ æ–‡ä»¶ç»“æ„:")
            for file_path, file_info in data['file_structure'].items():
                if file_info['type'] == 'file' and file_path.endswith('.py'):
                    analysis.append(f"  - {file_path} ({file_info['size']} bytes)")
            
            return "\n".join(analysis)
            
        except Exception as e:
            return f"åˆ†ææŠ¥å‘Šå¤±è´¥: {str(e)}"


class LogAnalysisInput(BaseModel):
    log_file_path: str = Field(..., description="è¦åˆ†æçš„æ—¥å¿—æ–‡ä»¶è·¯å¾„")
    log_format: str = Field(default="auto", description="æ—¥å¿—æ ¼å¼ (csv/txt/auto)")
    output_file: str = Field(default="", description="åˆ†æç»“æœè¾“å‡ºæ–‡ä»¶ (å¯é€‰)")


class LogAnalysisTool(BaseTool):
    """executionæ—¥å¿—analyzetool"""
    name: str = "log_analyzer"
    description: str = "åˆ†æAgentç³»ç»Ÿçš„æ‰§è¡Œæ—¥å¿—ï¼Œæ£€æµ‹é”™è¯¯å’Œå¼‚å¸¸æ¨¡å¼"
    args_schema: Type[BaseModel] = LogAnalysisInput

    def _run(self, log_file_path: str, log_format: str = "auto", output_file: str = "") -> str:
        try:
            if not Path(log_file_path).exists():
                return f"âŒ é”™è¯¯: æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ '{log_file_path}'"
            
            # createanalyzeå™¨
            analyzer = ExecutionLogAnalyzer()
            
            # analyzeæ—¥å¿—
            analysis_result = analyzer.analyze_log_file(log_file_path, log_format)
            
            # buildresultreport
            report_lines = []
            report_lines.append("=== æ‰§è¡Œæ—¥å¿—åˆ†ææŠ¥å‘Š ===\n")
            
            # åŸºæœ¬ç»Ÿè®¡
            stats = analysis_result.statistics
            report_lines.append("ğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
            report_lines.append(f"  - æ‰§è¡Œè·¯å¾„: {len(analysis_result.execution_paths)}")
            report_lines.append(f"  - é”™è¯¯æ•°é‡: {len(analysis_result.errors)}")
            report_lines.append(f"  - è­¦å‘Šæ•°é‡: {len(analysis_result.warnings)}")
            report_lines.append(f"  - æ—¥å¿—æ¡ç›®: {stats.get('total_entries', 0)}")
            report_lines.append(f"  - æ´»è·ƒAgents: {stats.get('active_agents', 0)}")
            report_lines.append("")
            
            # é”™è¯¯information
            if analysis_result.errors:
                report_lines.append("âŒ å‘ç°çš„é”™è¯¯:")
                for i, error in enumerate(analysis_result.errors[:10], 1):  # æœ€å¤šæ˜¾ç¤º10ä¸ª
                    report_lines.append(f"  {i}. {error.error_type} ({error.severity.value.upper()})")
                    report_lines.append(f"     èŠ‚ç‚¹: {error.node_or_edge}")
                    report_lines.append(f"     æè¿°: {error.description}")
                    if error.suggested_fix:
                        report_lines.append(f"     å»ºè®®: {error.suggested_fix}")
                    report_lines.append("")
                
                if len(analysis_result.errors) > 10:
                    report_lines.append(f"  ... è¿˜æœ‰ {len(analysis_result.errors) - 10} ä¸ªé”™è¯¯")
                report_lines.append("")
            
            # è­¦å‘Šinformation
            if analysis_result.warnings:
                report_lines.append("âš ï¸  è­¦å‘Šä¿¡æ¯:")
                for i, warning in enumerate(analysis_result.warnings[:5], 1):
                    report_lines.append(f"  {i}. {warning}")
                if len(analysis_result.warnings) > 5:
                    report_lines.append(f"  ... è¿˜æœ‰ {len(analysis_result.warnings) - 5} ä¸ªè­¦å‘Š")
                report_lines.append("")
            
            # æ”¹è¿›å»ºè®®
            if analysis_result.recommendations:
                report_lines.append("ğŸ’¡ æ”¹è¿›å»ºè®®:")
                for i, rec in enumerate(analysis_result.recommendations[:5], 1):
                    report_lines.append(f"  {i}. {rec}")
                if len(analysis_result.recommendations) > 5:
                    report_lines.append(f"  ... è¿˜æœ‰ {len(analysis_result.recommendations) - 5} ä¸ªå»ºè®®")
                report_lines.append("")
            
            # executionpathæ¦‚è§ˆ
            if analysis_result.execution_paths:
                report_lines.append("ğŸ›£ï¸  æ‰§è¡Œè·¯å¾„æ¦‚è§ˆ:")
                for i, path in enumerate(analysis_result.execution_paths[:3], 1):
                    report_lines.append(f"  è·¯å¾„ {i}: {path.path_id}")
                    report_lines.append(f"    èŠ‚ç‚¹æ•°: {len(path.nodes)}")
                    report_lines.append(f"    çŠ¶æ€: {path.status}")
                    if path.start_time and path.end_time:
                        duration = path.end_time - path.start_time
                        report_lines.append(f"    è€—æ—¶: {duration}")
                    report_lines.append("")
            
            # æ€»ç»“
            if analysis_result.summary:
                report_lines.append("ğŸ“‹ åˆ†ææ€»ç»“:")
                report_lines.append(f"  {analysis_result.summary}")
                report_lines.append("")
            
            # savedetailedresult
            if output_file:
                analysis_dict = {
                    'execution_paths': [
                        {
                            'path_id': path.path_id,
                            'nodes': path.nodes,
                            'edges': path.edges,
                            'start_time': path.start_time.isoformat() if path.start_time else None,
                            'end_time': path.end_time.isoformat() if path.end_time else None,
                            'status': path.status,
                            'log_entries_count': len(path.log_entries)
                        }
                        for path in analysis_result.execution_paths
                    ],
                    'errors': [
                        {
                            'error_type': error.error_type,
                            'severity': error.severity.value,
                            'description': error.description,
                            'node_or_edge': error.node_or_edge,
                            'suggested_fix': error.suggested_fix,
                            'context': error.context
                        }
                        for error in analysis_result.errors
                    ],
                    'warnings': analysis_result.warnings,
                    'statistics': analysis_result.statistics,
                    'recommendations': analysis_result.recommendations,
                    'summary': analysis_result.summary
                }
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(analysis_dict, f, indent=2, ensure_ascii=False)
                
                report_lines.append(f"ğŸ’¾ è¯¦ç»†åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
            
            return "\n".join(report_lines)
            
        except Exception as e:
            return f"æ—¥å¿—åˆ†æå¤±è´¥: {str(e)}"
