#!/usr/bin/env python3
"""
SentinelAgent - å‘½ä»¤è¡Œæ¥å£
AI Agentç³»ç»Ÿåˆ†æä¸ç›‘æ§å¹³å°çš„å‘½ä»¤è¡Œå·¥å…·
"""

import sys
import json
from pathlib import Path
from .scanner import scan_directory, scan_file
from .graph_builder import build_graph_from_scan, build_and_save_graph, scan_and_build_graph
from .path_analyzer import analyze_graph_paths, analyze_and_save_paths, analyze_paths_from_file
from .log_analyzer import ExecutionLogAnalyzer
from ..utils.path_resolver import resolve_path


def print_usage():
    """æ‰“å°ä½¿ç”¨è¯´æ˜"""
    print("ğŸ” Inspector Agent - Agentç³»ç»Ÿç»“æ„æ‰«æå™¨")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("  python cli.py <ç›®æ ‡è·¯å¾„> [é€‰é¡¹]")
    print("  python cli.py --analyze-graph <å›¾æ–‡ä»¶> [é€‰é¡¹]")
    print("  python cli.py --analyze-logs <æ—¥å¿—æ–‡ä»¶> [é€‰é¡¹]")
    print("\nå‚æ•°:")
    print("  ç›®æ ‡è·¯å¾„          è¦æ‰«æçš„ç›®å½•æˆ–æ–‡ä»¶è·¯å¾„")
    print("\né€‰é¡¹:")
    print("  -o, --output     æŒ‡å®šæ‰«æè¾“å‡ºæ–‡ä»¶å (é»˜è®¤: scan_result.json)")
    print("  -g, --graph      åŒæ—¶æ„å»ºå…³ç³»å›¾å¹¶ä¿å­˜ (éœ€è¦æŒ‡å®šå›¾è¾“å‡ºæ–‡ä»¶å)")
    print("  -p, --paths      åŒæ—¶è¿›è¡Œè·¯å¾„åˆ†æå¹¶ä¿å­˜ (éœ€è¦æŒ‡å®šè·¯å¾„åˆ†æè¾“å‡ºæ–‡ä»¶å)")
    print("  -a, --all        æ‰§è¡Œå®Œæ•´åˆ†æ (æ‰«æ+å›¾æ„å»º+è·¯å¾„åˆ†æ)")
    print("  --analyze-graph  åˆ†æå·²æœ‰çš„å›¾æ–‡ä»¶")
    print("  --analyze-logs   åˆ†ææ‰§è¡Œæ—¥å¿—æ–‡ä»¶")
    print("  --log-format     æŒ‡å®šæ—¥å¿—æ ¼å¼ (csv/txt/auto, é»˜è®¤: auto)")
    print("  --log-output     æŒ‡å®šæ—¥å¿—åˆ†æè¾“å‡ºæ–‡ä»¶")
    print("  -v, --verbose    æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯")
    print("  -h, --help       æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯")
    print("\nç¤ºä¾‹:")
    print("  python cli.py ../crewai_gmail")
    print("  python cli.py ../crewai_gmail --output gmail_scan.json")
    print("  python cli.py ../crewai_gmail --graph gmail_graph.json")
    print("  python cli.py ../crewai_gmail --paths gmail_paths.json") 
    print("  python cli.py ../crewai_gmail --all")
    print("  python cli.py --analyze-graph gmail_graph.json")
    print("  python cli.py --analyze-logs ../autogen_magneticone/logs/log_2025-05-17_17-47-03.txt")
    print("  python cli.py --analyze-logs ../magentic-one-file-code-execution.csv --log-format csv")
    print("  python cli.py tools.py --verbose")
    print("\næ³¨æ„:")
    print("  â€¢ ç›¸å¯¹è·¯å¾„å¦‚ '../crewai_gmail' åœ¨Dockerå®¹å™¨ä¸­ä¼šè‡ªåŠ¨è§£æ")
    print("  â€¢ å®¹å™¨ä¸­çš„è·¯å¾„ä¼šæ˜ å°„åˆ° /app/projects/ ç›®å½•ä¸‹")


def format_summary(summary):
    """æ ¼å¼åŒ–æ‘˜è¦ä¿¡æ¯"""
    return f"""ğŸ“Š æ‰«ææ‘˜è¦:
  ğŸ¤– Agents: {summary['total_agents']}
  ğŸ”§ Tools: {summary['total_tools']}
  ğŸ‘¥ Crews: {summary['total_crews']}
  ğŸ“‹ Tasks: {summary['total_tasks']}
  ğŸ“„ Files: {summary['total_files']}"""


def format_details(result, verbose=False):
    """æ ¼å¼åŒ–è¯¦ç»†ä¿¡æ¯"""
    details = []
    
    if result['agents'] and verbose:
        details.append("\nğŸ¤– å‘ç°çš„Agents:")
        for agent in result['agents'][:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
            details.append(f"  - {agent['name']} ({agent['type']})")
            if 'role' in agent.get('arguments', {}):
                details.append(f"    è§’è‰²: {agent['arguments']['role']}")
    
    if result['tools'] and verbose:
        details.append("\nğŸ”§ å‘ç°çš„Tools:")
        for tool in result['tools'][:10]:  # æœ€å¤šæ˜¾ç¤º10ä¸ª
            details.append(f"  - {tool['name']} ({tool['type']})")
    
    return "\n".join(details)


def format_path_summary(path_analysis):
    """æ ¼å¼åŒ–è·¯å¾„åˆ†ææ‘˜è¦"""
    overall = path_analysis.get('overall_assessment', {})
    return f"""ğŸ›£ï¸  è·¯å¾„åˆ†ææ‘˜è¦:
  ğŸ¯ æ€»ä½“é£é™©è¯„åˆ†: {overall.get('total_risk_score', 0):.3f}
  âš ï¸  é£é™©ç­‰çº§: {overall.get('risk_level', 'unknown').upper()}
  ğŸ“Š åˆ†æè·¯å¾„æ•°: {overall.get('total_paths_analyzed', 0)}
  ğŸš¨ å¯ç–‘æ¨¡å¼æ•°: {overall.get('suspicious_patterns_found', 0)}"""


def format_path_details(path_analysis, verbose=False):
    """æ ¼å¼åŒ–è·¯å¾„åˆ†æè¯¦ç»†ä¿¡æ¯"""
    details = []
    
    if verbose:
        # æ˜¾ç¤ºè·¯å¾„ç±»å‹åˆ†å¸ƒ
        path_types = path_analysis.get('path_analysis', {}).get('path_type_distribution', {})
        if path_types:
            details.append("\nğŸ›£ï¸  è·¯å¾„ç±»å‹åˆ†å¸ƒ:")
            for path_type, count in path_types.items():
                details.append(f"  - {path_type}: {count}")
        
        # æ˜¾ç¤ºå¯ç–‘æ¨¡å¼
        patterns = path_analysis.get('suspicious_patterns', [])
        if patterns:
            details.append("\nğŸš¨ å‘ç°çš„å¯ç–‘æ¨¡å¼:")
            for i, pattern in enumerate(patterns[:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
                details.append(f"  {i}. {pattern.get('pattern_type', 'unknown')} "
                             f"(ä¸¥é‡ç¨‹åº¦: {pattern.get('severity', 'unknown')})")
                details.append(f"     {pattern.get('details', '')}")
            if len(patterns) > 5:
                details.append(f"  ... è¿˜æœ‰ {len(patterns) - 5} ä¸ªæ¨¡å¼")
        
        # æ˜¾ç¤ºå»ºè®®
        recommendations = path_analysis.get('recommendations', [])
        if recommendations:
            details.append("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(recommendations[:3], 1):  # æœ€å¤šæ˜¾ç¤º3ä¸ª
                details.append(f"  {i}. {rec}")
            if len(recommendations) > 3:
                details.append(f"  ... è¿˜æœ‰ {len(recommendations) - 3} ä¸ªå»ºè®®")
    
    return "\n".join(details)


def format_log_summary(analysis_result):
    """æ ¼å¼åŒ–æ—¥å¿—åˆ†ææ‘˜è¦"""
    stats = analysis_result.statistics
    return f"""ğŸ“Š æ—¥å¿—åˆ†ææ‘˜è¦:
  ğŸ›£ï¸  æ‰§è¡Œè·¯å¾„: {len(analysis_result.execution_paths)}
  âŒ é”™è¯¯æ•°é‡: {len(analysis_result.errors)}
  âš ï¸  è­¦å‘Šæ•°é‡: {len(analysis_result.warnings)}
  ğŸ“ æ—¥å¿—æ¡ç›®: {stats.get('total_entries', 0)}
  ğŸ¤– æ´»è·ƒAgents: {stats.get('active_agents', 0)}
  â±ï¸  æ‰§è¡Œæ—¶é•¿: {stats.get('total_duration', 'N/A')}"""


def format_log_details(analysis_result, verbose=False):
    """æ ¼å¼åŒ–æ—¥å¿—åˆ†æè¯¦ç»†ä¿¡æ¯"""
    details = []
    
    if verbose and analysis_result.errors:
        details.append("\nâŒ å‘ç°çš„é”™è¯¯:")
        for i, error in enumerate(analysis_result.errors[:10], 1):  # æœ€å¤šæ˜¾ç¤º10ä¸ª
            details.append(f"  {i}. {error.error_type} ({error.severity.value.upper()})")
            details.append(f"     èŠ‚ç‚¹: {error.node_or_edge}")
            details.append(f"     æè¿°: {error.description}")
            if error.suggested_fix:
                details.append(f"     å»ºè®®: {error.suggested_fix}")
    
    if verbose and analysis_result.warnings:
        details.append("\nâš ï¸  è­¦å‘Šä¿¡æ¯:")
        for i, warning in enumerate(analysis_result.warnings[:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            details.append(f"  {i}. {warning}")
    
    if verbose and analysis_result.recommendations:
        details.append("\nğŸ’¡ æ”¹è¿›å»ºè®®:")
        for i, rec in enumerate(analysis_result.recommendations[:5], 1):  # æœ€å¤šæ˜¾ç¤º5ä¸ª
            details.append(f"  {i}. {rec}")
    
    return "\n".join(details)


def main():
    """ä¸»å‡½æ•°"""
    args = sys.argv[1:]
    
    # è§£æå‚æ•°
    if not args or '--help' in args or '-h' in args:
        print_usage()
        return
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†æå›¾æ–‡ä»¶æ¨¡å¼
    if '--analyze-graph' in args:
        return analyze_graph_mode(args)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†ææ—¥å¿—æ–‡ä»¶æ¨¡å¼
    if '--analyze-logs' in args:
        return analyze_logs_mode(args)
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†ææ—¥å¿—æ–‡ä»¶æ¨¡å¼
    if '--analyze-logs' in args:
        return analyze_logs_mode(args)
    
    target_path = args[0]
    output_file = "scan_result.json"
    graph_file = None
    paths_file = None
    verbose = False
    full_analysis = False
    
    # è§£æé€‰é¡¹
    i = 1
    while i < len(args):
        if args[i] in ['-o', '--output'] and i + 1 < len(args):
            output_file = args[i + 1]
            i += 2
        elif args[i] in ['-g', '--graph'] and i + 1 < len(args):
            graph_file = args[i + 1]
            i += 2
        elif args[i] in ['-p', '--paths'] and i + 1 < len(args):
            paths_file = args[i + 1]
            i += 2
        elif args[i] in ['-a', '--all']:
            full_analysis = True
            i += 1
        elif args[i] in ['-v', '--verbose']:
            verbose = True
            i += 1
        else:
            i += 1
    
    # å¦‚æœå¯ç”¨å®Œæ•´åˆ†æï¼Œè‡ªåŠ¨è®¾ç½®è¾“å‡ºæ–‡ä»¶å
    if full_analysis:
        base_name = Path(target_path).name
        if not graph_file:
            graph_file = f"graph_{base_name}.json"
        if not paths_file:
            paths_file = f"paths_{base_name}.json"
     # æ£€æŸ¥ç›®æ ‡è·¯å¾„ - æ”¯æŒè·¯å¾„è§£æ
    original_path = target_path
    resolved_path = resolve_path(target_path)
    path = Path(resolved_path)
    
    if not path.exists():
        print(f"âŒ é”™è¯¯: è·¯å¾„ä¸å­˜åœ¨ '{original_path}' (è§£æä¸º: {resolved_path})")
        return

    # æ‰§è¡Œæ‰«æ
    print(f"ğŸ” æ­£åœ¨æ‰«æ: {original_path}")
    if original_path != resolved_path:
        print(f"    è§£æè·¯å¾„: {resolved_path}")
    print("-" * 50)

    try:
        if path.is_dir():
            result = scan_directory(resolved_path)
            scan_type = "ç›®å½•"
        else:
            result = scan_file(resolved_path)
            scan_type = "æ–‡ä»¶"
        
        # æ˜¾ç¤ºæ‰«æç»“æœ
        print(f"âœ… {scan_type}æ‰«æå®Œæˆ!")
        print(format_summary(result['scan_summary']))
        
        if verbose:
            print(format_details(result, verbose=True))
        
        # ä¿å­˜æ‰«æç»“æœ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ æ‰«æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æ„å»ºå¹¶ä¿å­˜å›¾ï¼ˆå¦‚æœæŒ‡å®šäº†å›¾è¾“å‡ºæ–‡ä»¶ï¼‰
        graph_data = None
        if graph_file or paths_file:
            print(f"\nğŸ”— æ­£åœ¨æ„å»ºå…³ç³»å›¾...")
            graph_data = build_and_save_graph(result, graph_file or "temp_graph.json")
            print(f"âœ… å…³ç³»å›¾æ„å»ºå®Œæˆ!")
            print(f"ğŸ“Š å›¾ç»Ÿè®¡: {graph_data['graph_summary']['total_nodes']} ä¸ªèŠ‚ç‚¹, "
                  f"{graph_data['graph_summary']['total_edges']} æ¡è¾¹")
            if graph_file:
                print(f"ğŸ’¾ å…³ç³»å›¾å·²ä¿å­˜åˆ°: {graph_file}")
        
        # è·¯å¾„åˆ†æï¼ˆå¦‚æœæŒ‡å®šäº†è·¯å¾„è¾“å‡ºæ–‡ä»¶æˆ–å¯ç”¨äº†å®Œæ•´åˆ†æï¼‰
        if paths_file:
            print(f"\nğŸ›£ï¸  æ­£åœ¨åˆ†æè·¯å¾„...")
            if graph_data is None:
                # å¦‚æœè¿˜æ²¡æœ‰æ„å»ºå›¾ï¼Œå…ˆæ„å»º
                graph_data = build_graph_from_scan(result)
            
            path_analysis = analyze_and_save_paths(graph_data, paths_file)
            print(f"âœ… è·¯å¾„åˆ†æå®Œæˆ!")
            print(format_path_summary(path_analysis))
            
            if verbose:
                print(format_path_details(path_analysis, verbose=True))
            
            print(f"ğŸ’¾ è·¯å¾„åˆ†æå·²ä¿å­˜åˆ°: {paths_file}")
        
        # æç¤ºä¿¡æ¯
        if not verbose and (result['agents'] or result['tools']):
            print("ğŸ’¡ ä½¿ç”¨ --verbose é€‰é¡¹æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
        
        if not graph_file and result['scan_summary']['total_agents'] > 0:
            print("ğŸ’¡ ä½¿ç”¨ --graph é€‰é¡¹æ„å»ºç»„ä»¶å…³ç³»å›¾")
        
        if not paths_file and result['scan_summary']['total_agents'] > 0:
            print("ğŸ’¡ ä½¿ç”¨ --paths é€‰é¡¹è¿›è¡Œè·¯å¾„åˆ†æ")
        
        if not full_analysis and result['scan_summary']['total_agents'] > 0:
            print("ğŸ’¡ ä½¿ç”¨ --all é€‰é¡¹æ‰§è¡Œå®Œæ•´åˆ†æ")
    
    except Exception as e:
        print(f"âŒ æ‰«æå¤±è´¥: {e}")
        return 1


def analyze_graph_mode(args):
    """åˆ†æå›¾æ–‡ä»¶æ¨¡å¼"""
    graph_file = None
    paths_file = None
    verbose = False
    
    # æŸ¥æ‰¾å›¾æ–‡ä»¶å‚æ•°
    try:
        graph_idx = args.index('--analyze-graph')
        if graph_idx + 1 < len(args):
            graph_file = args[graph_idx + 1]
    except (ValueError, IndexError):
        print("âŒ é”™è¯¯: --analyze-graph éœ€è¦æŒ‡å®šå›¾æ–‡ä»¶è·¯å¾„")
        return 1
    
    # è§£æå…¶ä»–é€‰é¡¹
    i = 0
    while i < len(args):
        if args[i] in ['-p', '--paths'] and i + 1 < len(args):
            paths_file = args[i + 1]
            i += 2
        elif args[i] in ['-v', '--verbose']:
            verbose = True
            i += 1
        else:
            i += 1
    
    # è®¾ç½®é»˜è®¤è¾“å‡ºæ–‡ä»¶å
    if not paths_file:
        graph_path = Path(graph_file)
        paths_file = f"paths_{graph_path.stem}.json"
    
    # æ£€æŸ¥å›¾æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(graph_file).exists():
        print(f"âŒ é”™è¯¯: å›¾æ–‡ä»¶ä¸å­˜åœ¨ '{graph_file}'")
        return 1
    
    print(f"ğŸ›£ï¸  æ­£åœ¨åˆ†æå›¾æ–‡ä»¶: {graph_file}")
    print("-" * 50)
    
    try:
        # è¯»å–å›¾æ•°æ®å¹¶åˆ†æè·¯å¾„
        path_analysis = analyze_paths_from_file(graph_file, paths_file)
        
        print(f"âœ… è·¯å¾„åˆ†æå®Œæˆ!")
        print(format_path_summary(path_analysis))
        
        if verbose:
            print(format_path_details(path_analysis, verbose=True))
        
        print(f"ğŸ’¾ è·¯å¾„åˆ†æå·²ä¿å­˜åˆ°: {paths_file}")
        
        # æç¤ºä¿¡æ¯
        if not verbose:
            print("ğŸ’¡ ä½¿ç”¨ --verbose é€‰é¡¹æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
    
    except Exception as e:
        print(f"âŒ è·¯å¾„åˆ†æå¤±è´¥: {e}")
        return 1


def analyze_logs_mode(args):
    """åˆ†ææ—¥å¿—æ–‡ä»¶æ¨¡å¼"""
    log_file = None
    log_format = "auto"
    output_file = None
    verbose = False
    
    # æŸ¥æ‰¾æ—¥å¿—æ–‡ä»¶å‚æ•°
    try:
        log_idx = args.index('--analyze-logs')
        if log_idx + 1 < len(args):
            log_file = args[log_idx + 1]
    except (ValueError, IndexError):
        print("âŒ é”™è¯¯: --analyze-logs éœ€è¦æŒ‡å®šæ—¥å¿—æ–‡ä»¶è·¯å¾„")
        return 1
    
    # è§£æå…¶ä»–é€‰é¡¹
    i = 0
    while i < len(args):
        if args[i] == '--log-format' and i + 1 < len(args):
            log_format = args[i + 1]
            i += 2
        elif args[i] == '--log-output' and i + 1 < len(args):
            output_file = args[i + 1]
            i += 2
        elif args[i] in ['-v', '--verbose']:
            verbose = True
            i += 1
        else:
            i += 1
    
    # è®¾ç½®é»˜è®¤è¾“å‡ºæ–‡ä»¶å
    if not output_file:
        log_path = Path(log_file)
        output_file = f"analysis_{log_path.stem}.json"
    
    # æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(log_file).exists():
        print(f"âŒ é”™è¯¯: æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨ '{log_file}'")
        return 1
    
    print(f"ğŸ“Š æ­£åœ¨åˆ†ææ—¥å¿—æ–‡ä»¶: {log_file}")
    print(f"ğŸ“ æ—¥å¿—æ ¼å¼: {log_format}")
    print("-" * 50)
    
    try:
        # åˆ›å»ºåˆ†æå™¨å¹¶åˆ†ææ—¥å¿—
        analyzer = ExecutionLogAnalyzer()
        analysis_result = analyzer.analyze_log_file(log_file, log_format)
        
        print(f"âœ… æ—¥å¿—åˆ†æå®Œæˆ!")
        print(format_log_summary(analysis_result))
        
        if verbose:
            print(format_log_details(analysis_result, verbose=True))
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_dict = {
            'execution_paths': [
                {
                    'path_id': path.path_id,
                    'nodes': path.nodes,
                    'edges': path.edges,
                    'start_time': path.start_time.isoformat() if path.start_time else None,
                    'end_time': path.end_time.isoformat() if path.end_time else None,
                    'status': path.status,
                    'log_entries_count': len(path.log_entries)
                }
                for path in analysis_result.execution_paths
            ],
            'errors': [
                {
                    'error_type': error.error_type,
                    'severity': error.severity.value,
                    'description': error.description,
                    'node_or_edge': error.node_or_edge,
                    'suggested_fix': error.suggested_fix,
                    'context': error.context
                }
                for error in analysis_result.errors
            ],
            'warnings': analysis_result.warnings,
            'statistics': analysis_result.statistics,
            'recommendations': analysis_result.recommendations,
            'summary': analysis_result.summary
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_dict, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ åˆ†æç»“æœå·²ä¿å­˜åˆ°: {output_file}")
        
        # æç¤ºä¿¡æ¯
        if not verbose and (analysis_result.errors or analysis_result.warnings):
            print("ğŸ’¡ ä½¿ç”¨ --verbose é€‰é¡¹æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")
    
    except Exception as e:
        print(f"âŒ æ—¥å¿—åˆ†æå¤±è´¥: {e}")
        if verbose:
            import traceback
            traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main() or 0)
